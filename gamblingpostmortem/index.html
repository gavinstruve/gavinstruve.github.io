<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.81.0" />


<title>We built a machine learning model to gamble on the NBA Playoffs...Here&#39;s how it went - Gavin Struve&#39;s Github Site</title>
<meta property="og:title" content="We built a machine learning model to gamble on the NBA Playoffs...Here&#39;s how it went - Gavin Struve&#39;s Github Site">




  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/%3cnil%3e"
         width=""
         height=""
         alt="">
  </a>

  <ul class="nav-links">
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    

    <h1 class="article-title">We built a machine learning model to gamble on the NBA Playoffs...Here&#39;s how it went</h1>

    

    <div class="article-content">
      


<p>I have long thought NBA to be the most difficult major professional or collegiate sports league to gamble on.
Between players sitting out any given game with the new load management trend, the talent gap being a lot narrower than at the college level, and the sheer number of games, anyone can beat anyone else on any given night. But in the Playoffs, a lot of that volatility often disappears, at least that seemed to be the case this year as the better seed won all eight first round matchups.</p>
<p>So when two classmates and I were tasked with creating a machine learning model to predict the NBA Playoff games and gain a gambling edge, we set out with high hopes. By the end, we had experienced nearly all the highs and lows of gambling.</p>
<p>We based our model off of one group member’s NCAA Tournament predictive model that won the class challenge. It used a support vector machine with predictors that are universal to all levels of basketball: a team and opponent’s true shooting percentage, turnover percentage, offensive and defensive rebounding percentages, offensive efficiency, defensive efficiency and three point percentage. All the predictors used cumulative (seasonlong average) stats. We figured that to be a better picture than using a rolling (recency) average because a lot of players sit late season once a team locks up its playoff spot or wants to improve its lottery odds.</p>
<p>Despite our best efforts to find a combination of predictors to increase it, our model had a prediction accuracy of 62.3% for training data and 61.9% for testing data which is better than a coin flip but not a great confidence level. Our results reflected this. As such, we did not use our model as the end-all-be-all for making our gambling picks with our fictional money but as more of a guide or supplementary help. The model gave us a team’s winning percentage based on our predictors, and, with our professor’s help, we were able to display our model’s win percentage for a given team next to their betting odds within the code. We evaluated the disparities and gave credence to the model when it gave a team more favorable odds than Vegas did. The exception to this was when it seemed strangely weighted toward one team; we overrode the model in our decision-making a few times where we saw fit. We discussed with each other to reach a collective decision before making every pick and used the model and our existing NBA knowledge. The amounts we bet, which weren’t colossal until the last two dates, reflected our confidence in a decision.</p>
<p>At times, I thought my extensive NBA fandom/knowledge may be a hindrance as it could continually lead us against the model, but we were just as accurate using it as we were without. By my count, we used the model directly by picking the team it liked best on six picks, going 4-2, and deliberately went against it three times, going 2-1, so there didn’t appear to be much difference.</p>
<p>Despite leading the class and being in the green for a little while, we ended the exercise in last place at an overall loss but still with some of our money to spare. We actually won more bets than we lost (going 7-6 overall) but our final thousand dollar bet on the Timberwolves proved futile as we finished in a $719.82 deficit. This assignment taught me that human intervention is sometimes necessary and beneficial to use as a complement to a predictive model. It also taught me that it is perhaps beneficial to have a set amount to bet every time given that we won more bets than we lost but finished in a hole. Above all, the assignment reinforced that gambling is not for me and I am not in the small percentage of people capable of hitting over 60% of their bets and turning a profit. Until I can create a model with a higher prediction accuracy, I’ll stick to enjoying sports without finances at stake.</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

